<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Publications | Shivam Chaudhary</title> <meta name="author" content="Shivam Chaudhary"> <meta name="description" content="Research and Development Engineer working in brain-computer interfaces on non-human primates at the University of California Berkeley with Prof. Preeya Khanna. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img//assets/img/brain.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://shivam-199.github.io//publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//">Shivam Chaudhary</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching-mentoring/">Teaching &amp; Mentoring</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/bci_lstm_acm-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/bci_lstm_acm-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/bci_lstm_acm-1400.webp"></source> <img src="/assets/img/publication_preview/bci_lstm_acm.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="bci_lstm_acm.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="soni2024enhancing" class="col-sm-8"> <div class="title">Enhancing Motor Imagery based Brain Computer Interfaces for Stroke Rehabilitation</div> <div class="author"> Saher Soni, <em>Shivam Chaudhary</em>, and Krishna Prasad Miyapuram</div> <div class="periodical"> <em>In Proceedings of the 7th Joint International Conference on Data Science &amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Papers/Enhancing_Motor_Imagery_based_Brain_Computer_Interfaces_for_Stroke_rehabilitation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Globally, the prevalence of disabilities among stroke survivors exceeds 80%, with upper-limb movement impairments affecting over 85% of individuals. To address this challenge, motor imagery (MI) based brain-computer interface (BCI) has emerged as a promising approach for translating the imagined motor intentions of individuals into control signals for external devices. Electroencephalography (EEG) signals are commonly used in MI-BCIs due to their non-invasiveness, portability, high temporal resolution, and affordability. The present study utilized the publicly available Electroencephalography Motor Movement/Imagery Dataset (EEGMMIDB), comprising 64-channel EEG recordings from 109 participants sampled at 160 Hz. The aim was to classify between the opening/closing of palms and feet using the Long Short Term Memory (LSTM) network directly on cleaned EEG signals, bypassing traditional feature extraction methods that are computationally intensive and time-consuming. We achieved an average classification accuracy of 71.2% across subjects by tuning the hyperparameters related to epochs and segment length. This research emphasizes the efficacy of deep learning approaches in generating robust control signals for predicting motor intentions using EEG signals, eliminating the necessity of laborious feature extraction methods. By leveraging deep learning models, MI-BCI devices can advance neuro-rehabilitation, especially in stroke, by providing motor assistance, enabling patients to execute movements solely through the power of imagination.</p> </div> </div> </div> </li></ol> <h2 class="year">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/eeg-music-experiment-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/eeg-music-experiment-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/eeg-music-experiment-1400.webp"></source> <img src="/assets/img/publication_preview/eeg-music-experiment.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="eeg-music-experiment.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chaudhary2023predicting" class="col-sm-8"> <div class="title">Predicting drum beats from high-density Brain Rhythms</div> <div class="author"> <em>Shivam Chaudhary</em>, Krishna Prasad Miyapuram, and Derek Lomas</div> <div class="periodical"> <em>In Proceedings of the 6th Joint International Conference on Data Science &amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Papers/Predicting_drum_beats_from_high_density_Brain_Rhythms.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Entrainment is a phenomenon of phase or temporal matching of one system with that of another system. Human neural activity has been shown to resonate with external auditory stimuli. When we enjoy a piece of music, there is a resonance of brain responses with auditory signals. The crux of music cognition is based on this resonance of musical frequencies with intrinsic neural frequencies. It has also been demonstrated that the neural activities are synchronized across participants while listening to music, shown by high inter-subject correlation. In this work, we use this fact to predict the drumbeat a participant listens to based on their EEG response to the drumbeat. We also tested whether we could train on a smaller dataset and test with the rest of the dataset. We generated a frequency * channel plot and fed it to a CNN model to predict drumbeat with a classification accuracy of 97% for 60-20-20 (train-dev-test) data split protocol and 94% accuracy for 20-20-60 data split. We also got 100% classification accuracy for predicting participants for both the data split protocols.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/epilepsy-art-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/epilepsy-art-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/epilepsy-art-1400.webp"></source> <img src="/assets/img/publication_preview/epilepsy-art.jpg" class="preview z-depth-1 rounded" width="auto" height="auto" alt="epilepsy-art.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="rohira2023automatic" class="col-sm-8"> <div class="title">Automatic Epilepsy Detection from EEG signals</div> <div class="author"> Vridhi Rohira, <em>Shivam Chaudhary</em>, Sudip Das, and Krishna Prasad Miyapuram</div> <div class="periodical"> <em>In Proceedings of the 6th Joint International Conference on Data Science &amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Papers/Automatic_Epilepsy_Detection_from_EEG_signals.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Epilepsy is a neurological condition characterized by recurrent seizures and affects millions of people all over the world. The abnormal brain electrical activity during an epileptic seizure can be seen with an EEG, which is then read by a trained medical professional to diagnose epilepsy. However, this is often time-consuming, expensive, inaccessible, and inaccurate, thus highlighting the need for automated epilepsy prediction. Previous algorithms for this problem only made use of small data sets which lacked variable, clinical grade data. We used the TUEP dataset to extract features through power spectral density and power spectral connectivity. These features were then classified into epileptic vs non-epileptic using a random forest classifier. Our feature extraction methods using power spectral density and spectral connectivity showed accuracies of over 90% in detecting epilepsy.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/meditation_tradition_art-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/meditation_tradition_art-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/meditation_tradition_art-1400.webp"></source> <img src="/assets/img/publication_preview/meditation_tradition_art.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="meditation_tradition_art.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="10.1145/3571600.3571656" class="col-sm-8"> <div class="title">Towards the Development of Personalized and Generalized Interfaces for Brain Signals across Different Styles of Meditation</div> <div class="author"> Shruti Singh, Pankaj Pandey, <em>Shivam Chaudhary</em>, Krishna P Miyapuram, and James Lomas</div> <div class="periodical"> <em>In Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Papers/Towards_the_development_of_personalized_and_generalized_interfaces_for_brain_signals_across_different_styles_of_meditation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-doi="10.1145/3571600.3571656" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-doi="10.1145/3571600.3571656" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Human-computer interaction investigates how people learn from technology, and how they use technology in everyday life. Researchers have used brain-computer interfaces to understand how technology can be designed to support human cognition and behavior. The most famous and consumer-friendly approach to measuring brain signals is electroencephalography (EEG) due to its non-invasive, portable, relatively inexpensive, and high temporal resolution. In this study, we develop machine learning models to distinguish between the neural oscillations of meditators and non-meditators. Previous studies have used power spectrum density, entropy, and functional connectivity to distinguish various meditation traditions. We use EEG data set comprising neural activity of expert meditators of Himalayan Yoga (HYT), Vipassana (VIP), Isha Shoonya (SYN), and non-expert control subjects (CTR). We analyze the data using 13 different machine learning models for within-subject and cross-subject. We present the results for six classification conditions for both meditation and mind-wandering. Features extracted from the mean of 64 EEG time series are fed into machine learning classifiers during training. We obtain maximum accuracy for within-subject classification in both meditation and mind-wandering. In cross-subject analysis, we obtained 18.3% above chance level in meditation between control and Isha Shoonya, and similarly above 18% chance level in mind-wandering between control and Vipassana. We discuss the implications of this result for the emerging consumer EEG headset facilitating meditation practice. Our results indicate that personalized models (within-subject) and generalized models (cross-subject) could guide naive (beginner) practitioners to meditate and aim to modulate brain signals by practicing to reach the expert level.</p> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/meditation-art.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/meditation-art.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/meditation-art.PNG-1400.webp"></source> <img src="/assets/img/publication_preview/meditation-art.PNG" class="preview z-depth-1 rounded" width="auto" height="auto" alt="meditation-art.PNG" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chaudhary2022classifying" class="col-sm-8"> <div class="title">Classifying EEG signals of mind-wandering across different styles of meditation</div> <div class="author"> <em>Shivam Chaudhary</em>, Pankaj Pandey, Krishna Prasad Miyapuram, and Derek Lomas</div> <div class="periodical"> <em>In International Conference on Brain Informatics</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Papers/Classifying_EEG_signals_of_Mind_wandering_across_different_styles_of_Meditation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>In the modern world, it is easy to get lost in thought, partly because of the vast knowledge available at our fingertips via smartphones that divide our cognitive resources and partly because of our intrinsic thoughts. In this work, we aim to find the differences in the neural signatures of mind-wandering and meditation that are common across different meditative styles. We use EEG recordings done during meditation sessions by experts of different meditative styles, namely shamatha, zazen, dzogchen, and visualization. We evaluate the models using the leave-one-out validation technique to train on three meditative styles and test the fourth left-out style. With this method, we achieve an average classification accuracy of above 70%, suggesting that EEG signals of meditation techniques have a unique neural signature across meditative styles and can be differentiated from mind-wandering states. In addition, we generate lower-dimensional embeddings from higher-dimensional ones using t-SNE, PCA, and LLE algorithms and observe visual differences in embeddings between meditation and mind-wandering. We also discuss the general flow of the proposed design and contributions to the field of neuro-feedback-enabled mind-wandering detection and correction devices.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/meditation-types-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/meditation-types-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/meditation-types-1400.webp"></source> <img src="/assets/img/publication_preview/meditation-types.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="meditation-types.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pandey2022real" class="col-sm-8"> <div class="title">Real-time sensing and neurofeedback for practicing meditation using simultaneous EEG and eye tracking</div> <div class="author"> Pankaj Pandey, Pragati Gupta, <em>Shivam Chaudhary</em>, Krishna Prasad Miyapuram, and Derek Lomas</div> <div class="periodical"> <em>In 2022 IEEE Region 10 Symposium (TENSYMP)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Papers/Real_time_Sensing_and_NeuroFeedback_for_Practicing_Meditation_Using_Simultaneous_EEG_and_Eye_Tracking.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Next-generation intelligent health-monitoring systems have been ushered in by mobile phones with robust computation power, affordable high-resolution cameras, and embedded wearable sensors. Wearable Electroencephalography (EEG) head- set to record brain signals with simultaneous eye tracking using a mobile front camera can become a powerful tool to modulate brain activity for self-enhancement. Meditation has been proved to have great effects in neuroscientific investigations for decades. However, a naive practitioner initiates the practice with high motivation and steps down after struggling to get the feedback or follow the instructions. EEG headset and Eye-tracking technology can help beginners get real-time audio and visual feedback and encourage those who fail to continue a regular meditation session. This article proposes real-time feedback framework for generating mindful moments and trace progress while practicing. Three major components are: Learning Phase, Meditation Style Specific Feedback and Evaluation Phase. We discuss three learning moments including preparatory, disentanglement and mindful. This study facilitates the design of a neurofeedback product that can offer tailored feedback. Neurotechnological revolution enables individuals to attain better equilibrium, sustained attention, meta-cognitive awareness, decreased mind-wandering, and enhanced emotional stability through various meditation practices.</p> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Shivam Chaudhary. Last updated: October 16, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>