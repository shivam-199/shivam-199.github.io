<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Shivam Chaudhary</title> <meta name="author" content="Shivam Chaudhary"> <meta name="description" content="Research and Development Engineer working in brain-computer interfaces on non-human primates at the University of California Berkeley with Prof. Preeya Khanna. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img//assets/img/brain.ico"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://shivam-199.github.io//"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching-mentoring/">Teaching &amp; Mentoring</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Shivam Chaudhary </h1> <p class="desc">Research Engineer at <a href="https://neuralengatberkeley.github.io/" target="_blank" rel="external nofollow noopener">UC Berkeley EECS, Khanna Lab</a> | shivamc@berkeley.edu</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source> <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> <p>285 Li Ka Shing,</p> <p>UC Berkeley,</p> <p>Berkeley, CA - 94720</p> </div> </div> <div class="clearfix"> <p>Hi, I’m Shivam! I am a Research Engineer at the University of California at Berkeley EECS in Prof. Preeya Khanna’s Lab. I work on invasive brain-computer interfaces on non-human primates to understand the hand motor cortex.</p> <p>Previously, I was a senior postgraduate student in Cognitive Science at the Indian Institute of Technology Gandhinagar. I worked on decoding brain states using machine learning and deep learning approaches from electroencephalography (EEG) signals. I have also worked on building a cable-driven upper-limb exoskeleton. I have a bachelor’s in Computer Science and have worked as a full-stack software developer.</p> <p>In the future, I wish to pursue a PhD in invasive brain-computer interfaces to understand hand/leg motion and related fields.</p> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/meditation-art.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/meditation-art.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/meditation-art.PNG-1400.webp"></source> <img src="/assets/img/publication_preview/meditation-art.PNG" class="preview z-depth-1 rounded" width="auto" height="auto" alt="meditation-art.PNG" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chaudhary2022classifying" class="col-sm-8"> <div class="title">Classifying EEG signals of mind-wandering across different styles of meditation</div> <div class="author"> <em>Shivam Chaudhary</em>, Pankaj Pandey, Krishna Prasad Miyapuram, and Derek Lomas</div> <div class="periodical"> <em>In International Conference on Brain Informatics</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Papers/Classifying_EEG_signals_of_Mind_wandering_across_different_styles_of_Meditation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>In the modern world, it is easy to get lost in thought, partly because of the vast knowledge available at our fingertips via smartphones that divide our cognitive resources and partly because of our intrinsic thoughts. In this work, we aim to find the differences in the neural signatures of mind-wandering and meditation that are common across different meditative styles. We use EEG recordings done during meditation sessions by experts of different meditative styles, namely shamatha, zazen, dzogchen, and visualization. We evaluate the models using the leave-one-out validation technique to train on three meditative styles and test the fourth left-out style. With this method, we achieve an average classification accuracy of above 70%, suggesting that EEG signals of meditation techniques have a unique neural signature across meditative styles and can be differentiated from mind-wandering states. In addition, we generate lower-dimensional embeddings from higher-dimensional ones using t-SNE, PCA, and LLE algorithms and observe visual differences in embeddings between meditation and mind-wandering. We also discuss the general flow of the proposed design and contributions to the field of neuro-feedback-enabled mind-wandering detection and correction devices.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/eeg-music-experiment-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/eeg-music-experiment-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/eeg-music-experiment-1400.webp"></source> <img src="/assets/img/publication_preview/eeg-music-experiment.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="eeg-music-experiment.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chaudhary2023predicting" class="col-sm-8"> <div class="title">Predicting drum beats from high-density Brain Rhythms</div> <div class="author"> <em>Shivam Chaudhary</em>, Krishna Prasad Miyapuram, and Derek Lomas</div> <div class="periodical"> <em>In Proceedings of the 6th Joint International Conference on Data Science &amp; Management of Data (10th ACM IKDD CODS and 28th COMAD)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Papers/Predicting_drum_beats_from_high_density_Brain_Rhythms.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Entrainment is a phenomenon of phase or temporal matching of one system with that of another system. Human neural activity has been shown to resonate with external auditory stimuli. When we enjoy a piece of music, there is a resonance of brain responses with auditory signals. The crux of music cognition is based on this resonance of musical frequencies with intrinsic neural frequencies. It has also been demonstrated that the neural activities are synchronized across participants while listening to music, shown by high inter-subject correlation. In this work, we use this fact to predict the drumbeat a participant listens to based on their EEG response to the drumbeat. We also tested whether we could train on a smaller dataset and test with the rest of the dataset. We generated a frequency * channel plot and fed it to a CNN model to predict drumbeat with a classification accuracy of 97% for 60-20-20 (train-dev-test) data split protocol and 94% accuracy for 20-20-60 data split. We also got 100% classification accuracy for predicting participants for both the data split protocols.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/publication_preview/bci_lstm_acm-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/publication_preview/bci_lstm_acm-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/publication_preview/bci_lstm_acm-1400.webp"></source> <img src="/assets/img/publication_preview/bci_lstm_acm.png" class="preview z-depth-1 rounded" width="auto" height="auto" alt="bci_lstm_acm.png" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="soni2024enhancing" class="col-sm-8"> <div class="title">Enhancing Motor Imagery based Brain Computer Interfaces for Stroke Rehabilitation</div> <div class="author"> Saher Soni, <em>Shivam Chaudhary</em>, and Krishna Prasad Miyapuram</div> <div class="periodical"> <em>In Proceedings of the 7th Joint International Conference on Data Science &amp; Management of Data (11th ACM IKDD CODS and 29th COMAD)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="/assets/pdf/Papers/Enhancing_Motor_Imagery_based_Brain_Computer_Interfaces_for_Stroke_rehabilitation.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <span class="altmetric-embed" data-altmetric-id="" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span> <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 6px;"></span> </div> <div class="abstract hidden"> <p>Globally, the prevalence of disabilities among stroke survivors exceeds 80%, with upper-limb movement impairments affecting over 85% of individuals. To address this challenge, motor imagery (MI) based brain-computer interface (BCI) has emerged as a promising approach for translating the imagined motor intentions of individuals into control signals for external devices. Electroencephalography (EEG) signals are commonly used in MI-BCIs due to their non-invasiveness, portability, high temporal resolution, and affordability. The present study utilized the publicly available Electroencephalography Motor Movement/Imagery Dataset (EEGMMIDB), comprising 64-channel EEG recordings from 109 participants sampled at 160 Hz. The aim was to classify between the opening/closing of palms and feet using the Long Short Term Memory (LSTM) network directly on cleaned EEG signals, bypassing traditional feature extraction methods that are computationally intensive and time-consuming. We achieved an average classification accuracy of 71.2% across subjects by tuning the hyperparameters related to epochs and segment length. This research emphasizes the efficacy of deep learning approaches in generating robust control signals for predicting motor intentions using EEG signals, eliminating the necessity of laborious feature extraction methods. By leveraging deep learning models, MI-BCI devices can advance neuro-rehabilitation, especially in stroke, by providing motor assistance, enabling patients to execute movements solely through the power of imagination.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%73%68%69%76%61%6D%63@%62%65%72%6B%65%6C%65%79.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://github.com/shivam-199" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/shivam199" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> </div> <div class="contact-note"> Reach out to me for questions regarding my work or collaborations! </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Shivam Chaudhary. Last updated: October 16, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>